{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lucid Sonic Dreams - Music Visualization with GANs\n\nGenerate stunning AI music visualizations using StyleGAN3 and R3GAN.\n\n**Supported Models:**\n- **StyleGAN3** - High quality, many pretrained styles\n- **R3GAN** - Modern GAN baseline with SOTA FID scores (NeurIPS 2024)\n\n**Performance Features:**\n- FP16/BFloat16 inference (2x faster on GPU)\n- torch.compile() support (PyTorch 2.0+)\n- Custom style models (.pkl files)\n- Custom effects (zoom, swirl, etc.)\n- **uv package manager** (~10x faster installs)\n\n**Requirements:**\n- GPU runtime (Runtime > Change runtime type > T4 GPU)\n- Audio file (.mp3, .wav)\n- Python 3.12+"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "There are **over 30 parameters** you can tune, offering tons of flexibility. Here's a basic understanding:\n",
    "\n",
    "### The Process\n",
    "\n",
    "1. First, a batch of input vectors corresponding to output images is initialized. Linear interpolations between these vectors are produced, serving as the \"base\" vectors.\n",
    "2. Three components react to the audio: **Pulse**, **Motion**, and **Class**. These modify the \"base\" vectors accordingly.\n",
    "\n",
    "   * **Pulse** - How the visuals \"pulse\" to the beat. Reacts to percussive elements by default.\n",
    "   * **Motion** - How the visuals are \"pushed forward\" by the music. Reacts to harmonic elements by default.\n",
    "   * **Class** - Object labels in generated images (e.g., Van Gogh, Warhol for WikiArt). Reacts to audio pitch.\n",
    "\n",
    "3. Finally, additional effects (contrast, flash, custom) are applied.\n",
    "\n",
    "### Key Parameters to Start With\n",
    "\n",
    "Start by tuning these, then explore others:\n",
    "- **speed_fpm** - Frames Per Minute, controls morphing speed\n",
    "- **pulse_react** - Strength of pulse effect\n",
    "- **motion_react** - Strength of motion effect\n",
    "- **class_pitch_react** - How much pitch affects class changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "Run this cell to install all dependencies using uv (much faster than pip!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Install uv (fast Python package manager)\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "import os\n",
    "os.environ[\"PATH\"] = f\"/root/.local/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone lucid-sonic-dreams\n",
    "!git clone https://github.com/RayceRossum/lucid-sonic-dreams.git /content/lucid-sonic-dreams\n",
    "%cd /content/lucid-sonic-dreams\n",
    "\n",
    "# Install with uv (much faster than pip, ~10x speedup)\n",
    "!uv pip install --system -e .\n",
    "!uv pip install --system ninja  # For faster StyleGAN compilation\n",
    "\n",
    "# Setup StyleGAN3\n",
    "!git clone https://github.com/NVlabs/stylegan3 /content/lucid-sonic-dreams/stylegan3\n",
    "!ln -s /content/lucid-sonic-dreams/stylegan3 /content/lucid-sonic-dreams/stylegan2\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Installation complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive\n",
    "\n",
    "Mount to access your audio files, custom models, and save outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/LucidSonicDreams/outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check System & Available Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom lucidsonicdreams import (\n    show_styles, show_r3gan_styles, is_colab, suggest_batch_size,\n    get_gpu_memory_mb, get_free_gpu_memory_mb\n)\n\nprint(\"=\" * 50)\nprint(\"SYSTEM INFO\")\nprint(\"=\" * 50)\nprint(f\"Running on Colab: {is_colab()}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {get_gpu_memory_mb():.0f} MB total\")\n    print(f\"GPU Memory Free: {get_free_gpu_memory_mb():.0f} MB\")\nprint(f\"torch.compile available: {hasattr(torch, 'compile')}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"RECOMMENDED SETTINGS\")\nprint(\"=\" * 50)\nprint(f\"Batch size for 512px: {suggest_batch_size(512)}\")\nprint(f\"Batch size for 1024px: {suggest_batch_size(1024)}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"R3GAN MODELS (Modern GAN Baseline)\")\nprint(\"=\" * 50)\nshow_r3gan_styles()\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"STYLEGAN STYLES\")\nprint(\"=\" * 50)\nshow_styles()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Configure Your Visualization (Interactive)\n\nUse the dropdowns below to configure your visualization settings. After running this cell, modify the values and re-run the generation cell."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# ============================================================\n# Define available styles\n# ============================================================\nSTYLES = {\n    # StyleGAN styles\n    \"abstract\": \"abstract art\",\n    \"modern\": \"modern art\",\n    \"textures\": \"textures\",\n    \"microscope\": \"microscope images\",\n    \"landscapes\": \"landscape photos\",\n    \"faces\": \"photos of faces\",\n    \n    # R3GAN models (Modern GAN - NeurIPS 2024)\n    \"r3gan_faces_256\": \"r3gan_ffhq_256\",      # 256x256 faces (FID: 2.75)\n    \"r3gan_faces_64\": \"r3gan_ffhq_64\",        # 64x64 faces, fast (FID: 1.95)\n    \"r3gan_cifar\": \"r3gan_cifar10\",           # 32x32, 10 classes (FID: 1.96)\n    \"r3gan_imagenet_64\": \"r3gan_imagenet_64\", # 64x64, 1000 classes (FID: 2.09)\n    \"r3gan_imagenet_32\": \"r3gan_imagenet_32\", # 32x32, 1000 classes (FID: 1.27)\n    \n    # Add custom .pkl paths below:\n    # \"custom_wikiart\": \"/content/drive/MyDrive/models/wikiart.pkl\",\n}\n\n# ============================================================\n# Create interactive widgets\n# ============================================================\n\n# Style selection\nstyle_dropdown = widgets.Dropdown(\n    options=list(STYLES.keys()),\n    value='abstract',\n    description='Style:',\n    style={'description_width': '120px'},\n    layout=widgets.Layout(width='400px')\n)\n\n# Resolution\nresolution_dropdown = widgets.Dropdown(\n    options=[256, 512, 1024],\n    value=512,\n    description='Resolution:',\n    style={'description_width': '120px'},\n    layout=widgets.Layout(width='400px')\n)\n\n# FPS\nfps_dropdown = widgets.Dropdown(\n    options=[24, 30, 60],\n    value=30,\n    description='FPS:',\n    style={'description_width': '120px'},\n    layout=widgets.Layout(width='400px')\n)\n\n# Duration\nduration_slider = widgets.IntSlider(\n    value=30,\n    min=5,\n    max=300,\n    step=5,\n    description='Duration (sec):',\n    style={'description_width': '120px'},\n    layout=widgets.Layout(width='400px')\n)\n\n# Speed FPM\nspeed_slider = widgets.FloatSlider(\n    value=12,\n    min=0,\n    max=30,\n    step=1,\n    description='Speed (FPM):',\n    style={'description_width': '120px'},\n    layout=widgets.Layout(width='400px')\n)\n\n# Motion react\nmotion_slider = widgets.FloatSlider(\n    value=0.5,\n    min=0,\n    max=2,\n    step=0.1,\n    description='Motion React:',\n    style={'description_width': '120px'},\n    layout=widgets.Layout(width='400px')\n)\n\n# Pulse react\npulse_slider = widgets.FloatSlider(\n    value=0.5,\n    min=0,\n    max=2,\n    step=0.1,\n    description='Pulse React:',\n    style={'description_width': '120px'},\n    layout=widgets.Layout(width='400px')\n)\n\n# Truncation\ntruncation_slider = widgets.FloatSlider(\n    value=1.0,\n    min=0.1,\n    max=1.5,\n    step=0.1,\n    description='Truncation:',\n    style={'description_width': '120px'},\n    layout=widgets.Layout(width='400px')\n)\n\n# Effects toggles\ncontrast_checkbox = widgets.Checkbox(value=False, description='Contrast Effect')\nflash_checkbox = widgets.Checkbox(value=False, description='Flash Effect')\n\n# Effect strengths (shown when enabled)\ncontrast_strength = widgets.FloatSlider(value=0.5, min=0, max=1, step=0.1, description='Contrast:')\nflash_strength = widgets.FloatSlider(value=0.3, min=0, max=1, step=0.1, description='Flash:')\n\n# Audio reactivity presets\npreset_dropdown = widgets.Dropdown(\n    options=[\n        ('Custom', 'custom'),\n        ('Calm/Ambient', 'calm'),\n        ('Balanced', 'balanced'),\n        ('Energetic/EDM', 'energetic'),\n        ('Experimental', 'experimental'),\n    ],\n    value='balanced',\n    description='Preset:',\n    style={'description_width': '120px'},\n    layout=widgets.Layout(width='400px')\n)\n\ndef on_preset_change(change):\n    \"\"\"Apply preset values when preset changes.\"\"\"\n    preset = change['new']\n    if preset == 'calm':\n        speed_slider.value = 4\n        motion_slider.value = 0.2\n        pulse_slider.value = 0.2\n        truncation_slider.value = 0.8\n    elif preset == 'balanced':\n        speed_slider.value = 12\n        motion_slider.value = 0.5\n        pulse_slider.value = 0.5\n        truncation_slider.value = 1.0\n    elif preset == 'energetic':\n        speed_slider.value = 24\n        motion_slider.value = 1.0\n        pulse_slider.value = 1.2\n        truncation_slider.value = 1.0\n        contrast_checkbox.value = True\n        flash_checkbox.value = True\n    elif preset == 'experimental':\n        speed_slider.value = 6\n        motion_slider.value = 1.5\n        pulse_slider.value = 1.8\n        truncation_slider.value = 1.2\n        contrast_checkbox.value = True\n        flash_checkbox.value = True\n\npreset_dropdown.observe(on_preset_change, names='value')\n\n# Layout with sections\nvideo_settings = widgets.VBox([\n    widgets.HTML('<h4>ðŸ“¹ Video Settings</h4>'),\n    resolution_dropdown,\n    fps_dropdown,\n    duration_slider,\n])\n\nstyle_settings = widgets.VBox([\n    widgets.HTML('<h4>ðŸŽ¨ Style Settings</h4>'),\n    style_dropdown,\n    truncation_slider,\n])\n\naudio_settings = widgets.VBox([\n    widgets.HTML('<h4>ðŸŽµ Audio Reactivity</h4>'),\n    preset_dropdown,\n    speed_slider,\n    motion_slider,\n    pulse_slider,\n])\n\neffects_settings = widgets.VBox([\n    widgets.HTML('<h4>âœ¨ Effects</h4>'),\n    widgets.HBox([contrast_checkbox, flash_checkbox]),\n    contrast_strength,\n    flash_strength,\n])\n\n# Display all widgets\ndisplay(widgets.VBox([\n    widgets.HTML('<h3>ðŸŽ¬ Lucid Sonic Dreams Configuration</h3>'),\n    widgets.HBox([\n        widgets.VBox([video_settings, style_settings]),\n        widgets.VBox([audio_settings, effects_settings]),\n    ]),\n    widgets.HTML('<p><i>Tip: Select a preset to auto-configure settings, then fine-tune as needed.</i></p>')\n]))\n\n# Store current config for easy access\ndef get_config():\n    \"\"\"Get current configuration from widgets.\"\"\"\n    return {\n        'style': STYLES[style_dropdown.value],\n        'style_name': style_dropdown.value,\n        'resolution': resolution_dropdown.value,\n        'fps': fps_dropdown.value,\n        'duration': duration_slider.value,\n        'speed_fpm': speed_slider.value,\n        'motion_react': motion_slider.value,\n        'pulse_react': pulse_slider.value,\n        'truncation': truncation_slider.value,\n        'contrast_strength': contrast_strength.value if contrast_checkbox.value else None,\n        'flash_strength': flash_strength.value if flash_checkbox.value else None,\n    }\n\nprint(\"\\nâœ… Configuration widgets loaded! Adjust settings above, then run the generation cell.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select Audio Files\n",
    "\n",
    "Option A: Upload from your computer  \n",
    "Option B: Use files from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Upload audio file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Upload your audio file (.mp3 or .wav):\")\n",
    "uploaded = files.upload()\n",
    "AUDIO_FILE = list(uploaded.keys())[0]\n",
    "print(f\"\\nUploaded: {AUDIO_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Use files from Google Drive folder\n",
    "import glob\n",
    "import os\n",
    "\n",
    "AUDIO_FOLDER = \"/content/drive/MyDrive/LucidSonicDreams/audio/\"  # Edit this path\n",
    "\n",
    "# Find all audio files\n",
    "audio_files = glob.glob(os.path.join(AUDIO_FOLDER, \"*.mp3\")) + \\\n",
    "              glob.glob(os.path.join(AUDIO_FOLDER, \"*.wav\"))\n",
    "\n",
    "print(f\"Found {len(audio_files)} audio files:\")\n",
    "for i, f in enumerate(audio_files):\n",
    "    print(f\"  [{i}] {os.path.basename(f)}\")\n",
    "\n",
    "# Select file by index\n",
    "AUDIO_INDEX = 0  # Change this to select different file\n",
    "AUDIO_FILE = audio_files[AUDIO_INDEX] if audio_files else None\n",
    "print(f\"\\nSelected: {AUDIO_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Generate Video\n\nRun this cell to generate your video using the settings from the dropdowns above."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from lucidsonicdreams import LucidSonicDream\nimport os\n\n# Get configuration from widgets\nconfig = get_config()\n\nprint(\"=\" * 50)\nprint(\"GENERATION CONFIG\")\nprint(\"=\" * 50)\nfor k, v in config.items():\n    print(f\"  {k}: {v}\")\nprint(\"=\" * 50)\n\n# Create visualizer\nL = LucidSonicDream(\n    song=AUDIO_FILE,\n    style=config['style']\n)\n\n# Generate video\nOUTPUT_FILE = f\"{os.path.splitext(os.path.basename(AUDIO_FILE))[0]}_{config['style_name']}.mp4\"\n\nL.hallucinate(\n    file_name=OUTPUT_FILE,\n    resolution=config['resolution'],\n    fps=config['fps'],\n    duration=config['duration'],\n    batch_size=1,\n    speed_fpm=config['speed_fpm'],\n    motion_react=config['motion_react'],\n    pulse_react=config['pulse_react'],\n    truncation=config['truncation'],\n    contrast_strength=config['contrast_strength'],\n    flash_strength=config['flash_strength'],\n)\n\nprint(f\"\\nâœ… Video saved: {OUTPUT_FILE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preview & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Display video\n",
    "mp4 = open(OUTPUT_FILE, 'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(f'<video width=512 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download to your computer\n",
    "files.download(OUTPUT_FILE)\n",
    "\n",
    "# Or copy to Google Drive\n",
    "import shutil\n",
    "shutil.copy(OUTPUT_FILE, OUTPUT_DIR)\n",
    "print(f\"Copied to: {OUTPUT_DIR}/{OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Quality Render with All Parameters\n",
    "\n",
    "### Parameter Reference\n",
    "\n",
    "#### Initialization\n",
    "- **speed_fpm** (Default: 12) - Frames Per Minute. More = faster morphing. 0 = single reactive image.\n",
    "\n",
    "#### Pulse Parameters\n",
    "- **pulse_react** (Default: 0.5) - Strength of pulse. Keep between 0-2.\n",
    "- **pulse_percussive** (Default: True) - React to percussive elements.\n",
    "- **pulse_harmonic** (Default: False) - React to harmonic elements.\n",
    "- **pulse_audio** - Path to isolated audio track for pulse.\n",
    "\n",
    "#### Motion Parameters\n",
    "- **motion_react** (Default: 0.5) - Strength of motion. Keep between 0-2.\n",
    "- **motion_percussive** (Default: False) - React to percussive elements.\n",
    "- **motion_harmonic** (Default: True) - React to harmonic elements.\n",
    "- **motion_randomness** (Default: 0.5) - Randomness of motion. 0-1.\n",
    "- **truncation** (Default: 1) - Visual variety. Lower = less variety. 0-1.\n",
    "\n",
    "#### Class Parameters\n",
    "- **classes** - List of up to 12 numerical labels.\n",
    "- **dominant_classes_first** (Default: False) - Sort by prominence.\n",
    "- **class_pitch_react** (Default: 0.5) - Strength of pitch reaction. 0-2.\n",
    "- **class_smooth_seconds** (Default: 1) - Interpolation smoothness.\n",
    "- **class_complexity** (Default: 1) - Image complexity. 0-1.\n",
    "- **class_shuffle_seconds** - When to reshuffle class mapping.\n",
    "- **class_shuffle_strength** (Default: 0.5) - Shuffle intensity. 0-1.\n",
    "\n",
    "#### Effects Parameters\n",
    "- **contrast_strength** (Default: None) - Contrast effect strength. 0-1.\n",
    "- **contrast_percussive** (Default: True) - React to percussive.\n",
    "- **flash_strength** (Default: None) - Flash effect strength. 0-1.\n",
    "- **flash_percussive** (Default: True) - React to percussive.\n",
    "- **custom_effects** - List of EffectsGenerator objects.\n",
    "\n",
    "#### Video Parameters\n",
    "- **resolution** - Output resolution.\n",
    "- **start** (Default: 0) - Start timestamp in seconds.\n",
    "- **duration** - Duration in seconds. None = full audio.\n",
    "- **fps** (Default: 30) - Frames per second.\n",
    "- **output_audio** - Alternative audio for final video.\n",
    "- **batch_size** (Default: 1) - Vectors per batch.\n",
    "- **frame_batch_size** - Frames to process before writing to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lucidsonicdreams import LucidSonicDream\n",
    "\n",
    "# Full configuration example\n",
    "L = LucidSonicDream(\n",
    "    song=AUDIO_FILE,\n",
    "    style=STYLES[\"abstract\"],\n",
    "    # Optionally use separate audio tracks for each component:\n",
    "    # pulse_audio='drums.wav',\n",
    "    # motion_audio='other.wav',\n",
    "    # class_audio='melody.wav',\n",
    "    # contrast_audio='bass.wav',\n",
    "    # flash_audio='vocals.wav',\n",
    ")\n",
    "\n",
    "L.hallucinate(\n",
    "    file_name='full_quality.mp4',\n",
    "    \n",
    "    # Video settings\n",
    "    resolution=1024,\n",
    "    fps=60,\n",
    "    duration=None,  # Full song\n",
    "    start=0,\n",
    "    batch_size=1,\n",
    "    frame_batch_size=3000,  # Write to disk every N frames\n",
    "    \n",
    "    # Initialization\n",
    "    speed_fpm=6,\n",
    "    truncation=1.0,\n",
    "    \n",
    "    # Motion settings\n",
    "    motion_react=0.2,\n",
    "    motion_randomness=0.01,\n",
    "    motion_percussive=False,\n",
    "    motion_harmonic=True,\n",
    "    \n",
    "    # Pulse settings\n",
    "    pulse_react=0.15,\n",
    "    pulse_percussive=True,\n",
    "    pulse_harmonic=False,\n",
    "    \n",
    "    # Class settings (for models with classes like WikiArt)\n",
    "    # classes=[1, 5, 9, 16, 23, 27, 28, 30, 50, 68, 71, 89],\n",
    "    # dominant_classes_first=True,\n",
    "    # class_pitch_react=0.2,\n",
    "    class_smooth_seconds=2,\n",
    "    # class_shuffle_seconds=8,\n",
    "    # class_shuffle_strength=0.5,\n",
    "    # class_complexity=1.0,\n",
    "    \n",
    "    # Effects\n",
    "    # contrast_strength=0.5,\n",
    "    # contrast_percussive=True,\n",
    "    # flash_strength=0.3,\n",
    "    # flash_percussive=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Processing Multiple Songs\n",
    "\n",
    "Process multiple songs with different styles automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from lucidsonicdreams import LucidSonicDream\n",
    "\n",
    "# Configuration\n",
    "INPUT_FOLDER = \"/content/drive/MyDrive/LucidSonicDreams/audio/\"\n",
    "OUTPUT_FOLDER = \"/content/drive/MyDrive/LucidSonicDreams/outputs/\"\n",
    "\n",
    "# Find all audio files that don't have a corresponding .mp4\n",
    "song_data = []\n",
    "style_list = list(STYLES.values())\n",
    "\n",
    "for file_type in [\".mp3\", \".wav\"]:\n",
    "    for audio_file in glob.glob(os.path.join(INPUT_FOLDER, f\"*{file_type}\")):\n",
    "        video_file = audio_file.replace(file_type, '.mp4')\n",
    "        if not os.path.exists(video_file):\n",
    "            # Assign a random style or cycle through styles\n",
    "            style = random.choice(style_list)\n",
    "            song_data.append((audio_file, style))\n",
    "\n",
    "print(f\"Found {len(song_data)} songs to process:\")\n",
    "for song, style in song_data:\n",
    "    print(f\"  {os.path.basename(song)} -> {style}\")\n",
    "\n",
    "# Process each song\n",
    "for audio_path, style in song_data:\n",
    "    song_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "    output_name = f\"{song_name}.mp4\"\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing: {song_name}\")\n",
    "    print(f\"Style: {style}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    L = LucidSonicDream(audio_path, style=style)\n",
    "    \n",
    "    L.hallucinate(\n",
    "        file_name=output_name,\n",
    "        resolution=1024,\n",
    "        fps=60,\n",
    "        frame_batch_size=3000,\n",
    "        truncation=1,\n",
    "        duration=None,\n",
    "        speed_fpm=6,\n",
    "        batch_size=1,\n",
    "        motion_percussive=False,\n",
    "        motion_harmonic=True,\n",
    "        motion_react=0.2,\n",
    "        motion_randomness=0.01,\n",
    "        pulse_percussive=True,\n",
    "        pulse_harmonic=False,\n",
    "        pulse_react=0.15,\n",
    "        class_smooth_seconds=2,\n",
    "    )\n",
    "    \n",
    "    # Move to output folder\n",
    "    shutil.move(output_name, os.path.join(OUTPUT_FOLDER, output_name))\n",
    "    print(f\"Saved to: {OUTPUT_FOLDER}/{output_name}\")\n",
    "\n",
    "print(\"\\nBatch processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Custom Effects\n",
    "\n",
    "Create your own reactive effects! The effects function must have:\n",
    "- **array** - The image array to modify\n",
    "- **strength** - Reactivity parameter (0-1)\n",
    "- **amplitude** - Audio volume at current frame\n",
    "\n",
    "Return a NumPy array representing the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "from lucidsonicdreams import EffectsGenerator, LucidSonicDream\n",
    "\n",
    "def clipped_zoom(img, zoom_factor):\n",
    "    \"\"\"Zoom into image center without changing dimensions.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    zoom_tuple = (zoom_factor,) * 2 + (1,) * (img.ndim - 2)\n",
    "\n",
    "    if zoom_factor < 1:  # Zooming out\n",
    "        zh = int(np.round(h * zoom_factor))\n",
    "        zw = int(np.round(w * zoom_factor))\n",
    "        top = (h - zh) // 2\n",
    "        left = (w - zw) // 2\n",
    "        out = np.zeros_like(img)\n",
    "        out[top:top+zh, left:left+zw] = zoom(img, zoom_tuple)\n",
    "    elif zoom_factor > 1:  # Zooming in\n",
    "        zh = int(np.round(h / zoom_factor))\n",
    "        zw = int(np.round(w / zoom_factor))\n",
    "        top = (h - zh) // 2\n",
    "        left = (w - zw) // 2\n",
    "        out = zoom(img[top:top+zh, left:left+zw], zoom_tuple)\n",
    "        trim_top = ((out.shape[0] - h) // 2)\n",
    "        trim_left = ((out.shape[1] - w) // 2)\n",
    "        out = out[trim_top:trim_top+h, trim_left:trim_left+w]\n",
    "    else:\n",
    "        out = img\n",
    "    return out\n",
    "\n",
    "def zoom_effect_func(array, strength, amplitude):\n",
    "    \"\"\"Zoom effect that reacts to audio.\"\"\"\n",
    "    zoom_amount = 1 + (0.05 * strength * amplitude)\n",
    "    zoomed = clipped_zoom(array, zoom_amount)\n",
    "    return zoomed.astype(np.uint8)\n",
    "\n",
    "# Create the effect generator\n",
    "zoom_effect = EffectsGenerator(\n",
    "    func=zoom_effect_func,\n",
    "    audio=AUDIO_FILE,\n",
    "    strength=0.5,\n",
    "    percussive=True  # React to beats\n",
    ")\n",
    "\n",
    "# Use with visualization\n",
    "L = LucidSonicDream(AUDIO_FILE, style=STYLES[\"abstract\"])\n",
    "\n",
    "L.hallucinate(\n",
    "    file_name='with_zoom_effect.mp4',\n",
    "    resolution=512,\n",
    "    fps=30,\n",
    "    duration=30,\n",
    "    custom_effects=[zoom_effect]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swirl effect example\n",
    "from skimage.transform import swirl\n",
    "\n",
    "def swirl_effect_func(array, strength, amplitude):\n",
    "    \"\"\"Swirl effect that reacts to audio.\"\"\"\n",
    "    swirled = swirl(\n",
    "        array,\n",
    "        rotation=0,\n",
    "        strength=100 * strength * amplitude,\n",
    "        radius=650\n",
    "    )\n",
    "    return (swirled * 255).astype(np.uint8)\n",
    "\n",
    "swirl_effect = EffectsGenerator(\n",
    "    func=swirl_effect_func,\n",
    "    audio=AUDIO_FILE,\n",
    "    strength=0.2,\n",
    "    percussive=False  # React to harmonic content\n",
    ")\n",
    "\n",
    "L = LucidSonicDream(AUDIO_FILE, style=STYLES[\"textures\"])\n",
    "\n",
    "L.hallucinate(\n",
    "    file_name='with_swirl_effect.mp4',\n",
    "    resolution=512,\n",
    "    fps=30,\n",
    "    duration=30,\n",
    "    motion_react=0.15,\n",
    "    speed_fpm=2,\n",
    "    pulse_react=1.5,\n",
    "    contrast_strength=1,\n",
    "    flash_strength=1,\n",
    "    custom_effects=[swirl_effect]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Custom Visualization Functions (Advanced)\n",
    "\n",
    "Use any model that generates images from vectors! Example with BigGAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigGAN example (uncomment to use)\n",
    "# !pip install pytorch_pretrained_biggan\n",
    "\n",
    "# from pytorch_pretrained_biggan import BigGAN, convert_to_images\n",
    "# import torch\n",
    "\n",
    "# biggan = BigGAN.from_pretrained('biggan-deep-512')\n",
    "# biggan.to('cuda:0')\n",
    "\n",
    "# def biggan_func(noise_batch, class_batch):\n",
    "#     noise_tensor = torch.from_numpy(noise_batch).cuda()\n",
    "#     class_tensor = torch.from_numpy(class_batch).cuda()\n",
    "#     with torch.no_grad():\n",
    "#         output_tensor = biggan(noise_tensor.float(), class_tensor.float(), truncation=1)\n",
    "#     return convert_to_images(output_tensor.cpu())\n",
    "\n",
    "# # ImageNet class labels: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
    "# L = LucidSonicDream(\n",
    "#     AUDIO_FILE,\n",
    "#     style=biggan_func,\n",
    "#     input_shape=128,\n",
    "#     num_possible_classes=1000\n",
    "# )\n",
    "\n",
    "# L.hallucinate(\n",
    "#     file_name='biggan_output.mp4',\n",
    "#     resolution=512,\n",
    "#     duration=60,\n",
    "#     speed_fpm=3,\n",
    "#     classes=[13, 14, 22, 24, 301, 84, 99, 100, 134, 143, 393, 394],\n",
    "#     class_shuffle_seconds=10,\n",
    "#     class_shuffle_strength=0.1,\n",
    "#     class_complexity=0.5,\n",
    "#     class_smooth_seconds=4,\n",
    "#     motion_react=0.35,\n",
    "#     flash_strength=1,\n",
    "#     contrast_strength=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Clean temp files\n",
    "!rm -rf /tmp/lsd_*.mp4 /tmp/lsd_*.wav\n",
    "!rm -rf /content/stylegan2 /content/stylegan3\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "if torch.cuda.is_available():\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    print(f\"Free GPU memory: {free / 1024**2:.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Tips & Troubleshooting\n\n### Speed Optimizations (automatic)\n- FP16 inference for StyleGAN (2x faster on GPU)\n- BFloat16 inference for R3GAN (optimal precision)\n- torch.compile() on PyTorch 2.0+\n- torch.inference_mode()\n- Pre-allocated arrays\n- Vectorized NumPy operations\n\n### Manual Tuning\n- Lower resolution (512 vs 1024) for 4x faster generation\n- Lower FPS (24 vs 60) for 2.5x faster\n- Use `batch_size=2` if you have >12GB VRAM\n- Use `frame_batch_size` to write frames to disk periodically (prevents OOM)\n\n### Troubleshooting\n- **OOM errors**: Reduce resolution or batch_size, add frame_batch_size\n- **Slow generation**: Ensure GPU runtime is enabled\n- **Codec errors**: Library auto-tries aac, libmp3lame, mp3 codecs\n- **Style not found**: Use `show_styles()` or `show_r3gan_styles()` to see options\n\n### Resources\n- [Project GitHub](https://github.com/RayceRossum/lucid-sonic-dreams)\n- [StyleGAN3 Models](https://github.com/NVlabs/stylegan3)\n- [Pretrained StyleGAN Models](https://github.com/justinpinkney/awesome-pretrained-stylegan2)\n- [R3GAN Paper](https://arxiv.org/abs/2501.05441) - \"The GAN is dead; long live the GAN!\"\n- [R3GAN Models on HuggingFace](https://huggingface.co/brownvc)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}